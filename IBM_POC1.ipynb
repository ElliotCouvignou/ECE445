{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.07, 0.44) (0.44, 0.57) (0.57, 0.82) (0.82, 1.04) (1.04, 1.62)\n",
      " (1.62, 1.74) (1.74, 2.28) (2.28, 2.58) (2.58, 2.8) (2.8, 3.4)]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import wave\n",
    "import math\n",
    "import numpy as np\n",
    "import copy as copy\n",
    "#import DSP\n",
    "from pydub.utils import mediainfo\n",
    "from os.path import join, dirname\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_watson.websocket import RecognizeCallback, AudioSource\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from scipy import signal\n",
    "from audio2numpy import open_audio\n",
    "\n",
    "class MyRecognizeCallback(RecognizeCallback):\n",
    "    def __init__(self):\n",
    "        RecognizeCallback.__init__(self)\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print(json.dumps(data, indent=2))\n",
    "\n",
    "    def on_error(self, error):\n",
    "        print('Error received: {}'.format(error))\n",
    "\n",
    "    def on_inactivity_timeout(self, error):\n",
    "        print('Inactivity timeout: {}'.format(error))\n",
    "        \n",
    "class Transcript():\n",
    "    def __init__(self):\n",
    "        self.words = np.array(['wordwordwordwordwordword'])\n",
    "        self.timestamps = np.array([0.00], dtype=object)\n",
    "\n",
    "\n",
    "    def initAudio(self, audio, sr):\n",
    "        self.audio = audio\n",
    "        self.sr = sr\n",
    "        \n",
    "    def copyother(self, transcript):\n",
    "        self.words = copy.deepcopy(transcript.words)\n",
    "        self.timestamps = copy.deepcopy(transcript.timestamps)\n",
    "        \n",
    "    def setupIBM(self, transcript, confidence):\n",
    "        self.words = np.repeat(self.words, len(transcript))\n",
    "        self.timestamps = np.repeat(self.timestamps, len(transcript))\n",
    "        self.confidence = confidence\n",
    "        \n",
    "        i = 0\n",
    "        for word in transcript:\n",
    "            self.words[i] = word[0]\n",
    "            self.timestamps[i] = (word[1],word[2])\n",
    "            i += 1\n",
    "\n",
    "    def swap(self, i, j):\n",
    "        tmp = self.words[j]\n",
    "        self.words[j] = self.words[i]\n",
    "        self.words[i] = tmp\n",
    "\n",
    "        tmp = self.timestamps[j]\n",
    "        self.timestamps[j] = self.timestamps[i]\n",
    "        self.timestamps[i] = tmp\n",
    "\n",
    "    def getSpec(self):\n",
    "        spec = DSP.stft(input_sound=self.audio, dft_size=256, hop_size=64, zero_pad=256, window=signal.hann(256))\n",
    "        t,f = DSP.FormatAxis(spec, self.sr, len(self.audio)/self.sr)\n",
    "        return spec, t, f\n",
    "\n",
    "    # creates main channel type transcript from others. Basically combines them\n",
    "    def MainFromOthers(self, transcripts):\n",
    "        \n",
    "        for i in range(len(transcripts)):\n",
    "            transcript = transcripts[i]\n",
    "            if( i == 0):\n",
    "                self.words = transcript.words\n",
    "                self.timestamps = transcript.timestamps\n",
    "            else:\n",
    "                self.words = np.hstack((self.words, transcript.words))\n",
    "                self.timestamps = np.hstack((self.timestamps, transcript.timestamps))\n",
    "\n",
    "        self.quicksort()        \n",
    "        \n",
    "    #\n",
    "    # Transcript.words[i] = i-th word\n",
    "    # Transcript.timestamps[i] = start/end times for i-th word\n",
    "    #\n",
    "    def RenderTranscription(self, oldtrans, newtrans, windowing=False):\n",
    "        render = np.array([0])\n",
    "        renderlen = 0\n",
    "\n",
    "        newtime = newtrans.timestamps\n",
    "        oldtime = oldtrans.timestamps\n",
    "        # loop through each word, if this is latest word then extend render\n",
    "        idx = 0\n",
    "        for i in range(len(oldtrans.words)):\n",
    "\n",
    "            # get start/end times in samples for slicing\n",
    "            oldstart_n = time2sample(oldtime[i][0],oldtrans.sr)\n",
    "            oldend_n = time2sample(oldtime[i][1],  oldtrans.sr)\n",
    "            newstart_n = time2sample(newtime[i][0],oldtrans.sr)\n",
    "            newend_n = time2sample(newtime[i][1],  oldtrans.sr)    \n",
    "\n",
    "            shift = newstart_n - oldstart_n\n",
    "\n",
    "            if(newend_n > renderlen):  \n",
    "                # extend render length \n",
    "                l = newend_n - renderlen\n",
    "                pad = np.zeros(l)\n",
    "                if(renderlen == 0):\n",
    "                    render = pad\n",
    "                else:\n",
    "                    render = np.hstack((render, pad))\n",
    "                renderlen = len(render)\n",
    "\n",
    "            # place audio slice into render\n",
    "            if(windowing and shift != 0):\n",
    "                # ATM trying out Hamming for minimal spectral coloring\n",
    "                windowed = np.asarray(oldtrans.audio[oldstart_n:oldend_n], dtype=np.float)\n",
    "\n",
    "                delay_ms = round(.075 * oldtrans.sr) # 75 ms for now. based on feel\n",
    "                windowed[:delay_ms] *= np.linspace(0.0,1.0 ,min(delay_ms, len(windowed)))  # front\n",
    "                windowed[-delay_ms:] *= np.linspace(0.0,1.0 ,min(delay_ms, len(windowed)))  # end\n",
    "\n",
    "                render[newstart_n:newend_n] += windowed\n",
    "            else:\n",
    "                render[newstart_n:newend_n] += oldtrans.audio[oldstart_n:oldend_n]\n",
    "            idx += 1\n",
    "\n",
    "        newtrans.audio = render\n",
    "        newtrans.sr = oldtrans.sr\n",
    "        return render\n",
    "\n",
    "\n",
    "    # calls Render Transcription for each channel\n",
    "    # parameters are arrays where each index are parameters to individual render transcription calls\n",
    "    def RenderMultiChannels(self, oldtrans, newtrans, audios, srs, window=False):\n",
    "        for i in len(oldtrans):\n",
    "            self.RenderTranscription(oldtrans[i], newtrans[i], audios[i], srs[i], window)\n",
    "\n",
    "\n",
    "    \n",
    "    # next two are for sorting transcription words based on timestamps\n",
    "    def partition(self, low, high):\n",
    "        # We select the middle element to be the pivot. Some implementations select\n",
    "        # the first element or the last element. Sometimes the median value becomes\n",
    "        # the pivot, or a random one. There are many more strategies that can be\n",
    "        # chosen or created.\n",
    "        pivot = self.timestamps[(low + high) // 2][0]\n",
    "        i = low - 1\n",
    "        j = high + 1\n",
    "        while True:\n",
    "            i += 1\n",
    "            while self.timestamps[i][0] < pivot:\n",
    "                i += 1\n",
    "\n",
    "            j -= 1\n",
    "            while self.timestamps[j][0] > pivot:\n",
    "                j -= 1\n",
    "\n",
    "            if i >= j:\n",
    "                return j\n",
    "\n",
    "            # At this poimt i (on the left of the pivot) is larger than the\n",
    "            # element at j (on right right of the pivot)\n",
    "            self.swap(i, j)\n",
    "\n",
    "    def _quick_sort(self, low, high):\n",
    "        if low < high:\n",
    "            # This is the index after the pivot, where our lists are split\n",
    "            split_index = self.partition(low, high)\n",
    "            self._quick_sort(low, split_index)\n",
    "            self._quick_sort(split_index + 1, high)\n",
    "\n",
    "    def quicksort(self):\n",
    "        # Create a helper function that will be called recursively\n",
    "        self._quick_sort(0, len(self.timestamps) - 1)\n",
    "   \n",
    "    def ibm_recog(self,audioname,audiofp):\n",
    "        authenticator = IAMAuthenticator('6noBhxJHkbRVsgbxsl47v6dFZnJdoRRrDRYte7GgKKxu')\n",
    "        speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "        speech_to_text.set_service_url('https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/51085e72-7959-4c18-94cd-d4d874baf61d')\n",
    "        myRecognizeCallback = MyRecognizeCallback()\n",
    "    \n",
    "        with open(join(dirname(audioname), audiofp), 'rb') as audio_file:\n",
    "        \n",
    "            audio_source = AudioSource(audio_file)\n",
    "        \n",
    "            x = speech_to_text.recognize(\n",
    "                audio=audio_file,\n",
    "                content_type='audio/wav',\n",
    "                recognize_callback=myRecognizeCallback,\n",
    "                model='en-US_BroadbandModel',\n",
    "                timestamps=True,\n",
    "            #smart_formatting=True\n",
    "            )\n",
    "        result = x.result\n",
    "        alternatives = result.get('results')[0].get('alternatives')[0]\n",
    "        transcript = alternatives.get('transcript')\n",
    "        timestamps = alternatives.get('timestamps')\n",
    "        confidence = alternatives.get('confidence')\n",
    "        a,sr=open_audio(fpp)\n",
    "        self.initAudio(a,sr)\n",
    "        self.setupIBM(timestamps,confidence)\n",
    "\n",
    "\n",
    "\n",
    "def time2sample(time, sr):\n",
    "    return round(time*sr)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
