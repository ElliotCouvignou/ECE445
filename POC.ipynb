{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done sETUP\n"
     ]
    }
   ],
   "source": [
    "### THIS CELL IS JUST FOR 3RD PARTY FUNCTIONS ### \n",
    "def sound( x, rate=8000, label=''):\n",
    "    from IPython.display import display, Audio, HTML\n",
    "    if label is '':\n",
    "        display( Audio( x, rate=rate))\n",
    "    else:\n",
    "        display( HTML( \n",
    "        '<style> table, th, td {border: 0px; }</style> <table><tr><td>' + label + \n",
    "        '</td><td>' + Audio( x, rate=rate)._repr_html_()[3:] + '</td></tr></table>'\n",
    "        ))\n",
    "        \n",
    "        ## carry over code placed here to reduce clutter\n",
    "\n",
    "\n",
    "# i did make this tho\n",
    "def stft( input_sound, dft_size, hop_size, zero_pad, window=1.0):\n",
    "    length = len(input_sound)\n",
    "    \n",
    "    # Part1. splitting into frames\n",
    "    FrameAmount = math.ceil((length) / hop_size) + 1\n",
    "    slices = np.arange(dft_size * FrameAmount).reshape(dft_size, FrameAmount)\n",
    "    # set slices into array\n",
    "    for i in range(FrameAmount):\n",
    "        start = i * hop_size\n",
    "        end = start + dft_size\n",
    "        \n",
    "        data = input_sound[start:end]\n",
    "        \n",
    "        # input too short... need to zero padd end\n",
    "        if(data.shape[0] < dft_size):\n",
    "            zero_padd = np.zeros(dft_size - data.shape[0])\n",
    "            data = np.hstack((data, zero_padd))\n",
    "           \n",
    "        slices[:,i] = data * window\n",
    "        \n",
    "    #  Part2. Do fft of input slices        \n",
    "    size = dft_size+zero_pad   \n",
    "    if(size%2 ==0):\n",
    "        NumBins = ((size) // 2) + 1\n",
    "    else:\n",
    "        NumBins = ((size) + 1) // 2\n",
    "    \n",
    "    NumBins = int(NumBins)\n",
    "    f = np.arange(NumBins * FrameAmount, dtype=np.complex_).reshape(NumBins, FrameAmount)   \n",
    "    f[:,:] = 0. + 0.j\n",
    "    \n",
    "    for i in range(FrameAmount):\n",
    "        f[:,i] = np.fft.rfft(slices[:,i], size)      \n",
    "\n",
    "    # Return a complex-valued spectrogram (frequencies x time)\n",
    "    return f\n",
    "\n",
    "def FormatAxis(specArray, sr, time):\n",
    "    length = specArray.shape[1]\n",
    "    numbins = specArray.shape[0]\n",
    "    timeline = np.linspace(0, time, length)\n",
    "    freqline = np.linspace(0, sr/2, numbins)\n",
    "    #freqline = np.fft.fftfreq(numbins, d=1./sr)\n",
    "    return timeline, freqline\n",
    "\n",
    "def time2sample(time, sr):\n",
    "    return round(time*sr)\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "       return v\n",
    "    return v / norm\n",
    "\n",
    "\n",
    "print('Done sETUP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### REAL POC CELL ##\n",
    "\n",
    "\"\"\"\n",
    "Created on Mon Feb 10 16:36:55 2020\n",
    "No stealy \n",
    "@author: ecouv\n",
    "\"\"\"\n",
    "# Qt5 related stuff\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QMenu, QVBoxLayout, QSizePolicy, QMessageBox, QWidget, QPushButton\n",
    "from PyQt5.QtGui import QIcon\n",
    "\n",
    "from PyQt5 import QtWidgets\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'UserInterface/')\n",
    "from GUI import Ui_TranscriptEditor\n",
    "from Render import Transcript\n",
    "\n",
    "\n",
    "# transcription library stuff\n",
    "from google.cloud import speech_v1\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg') # for QT5\n",
    "\n",
    "# stuff\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def sample_long_running_recognize(storage_uri):\n",
    "    \"\"\"\n",
    "    Print start and end time of each word spoken in audio file from Cloud Storage\n",
    "\n",
    "    Args:\n",
    "      storage_uri URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]\n",
    "    \"\"\"\n",
    "\n",
    "    client = speech_v1.SpeechClient()\n",
    "\n",
    "    # storage_uri = 'gs://cloud-samples-data/speech/brooklyn_bridge.flac'\n",
    "\n",
    "    # When enabled, the first result returned by the API will include a list\n",
    "    # of words and the start and end time offsets (timestamps) for those words.\n",
    "    enable_word_time_offsets = True\n",
    "\n",
    "    # The language of the supplied audio\n",
    "    language_code = \"en-US\"\n",
    "    config = {\n",
    "        \"enable_word_time_offsets\": enable_word_time_offsets,\n",
    "        \"language_code\": language_code,\n",
    "    }\n",
    "    audio = {\"uri\": storage_uri}\n",
    "\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print(u\"Waiting for transcription to complete...\")\n",
    "    response = operation.result()\n",
    "\n",
    "    # The first result includes start and end time word offsets\n",
    "    result = response.results[0]\n",
    "    # First alternative is the most probable result\n",
    "    alternative = result.alternatives[0]\n",
    "    print(u\"Transcription complete...\")\n",
    " \n",
    "        \n",
    "    return alternative.words\n",
    "\n",
    "\n",
    "\n",
    "# shifts by unit of time in seconds\n",
    "def ShiftTranscriptWord(transcript, index, timeshift):\n",
    "    \n",
    "    secs = int(timeshift)\n",
    "    nanos = int((timeshift - secs) * 10**9)\n",
    "    \n",
    "    word = transcript[index]\n",
    "    \n",
    "    if(word.start_time.nanos + nanos >= 10**9):\n",
    "        secs += 1\n",
    "    if(word.end_time.nanos + nanos >= 10**9):\n",
    "        secs += 1\n",
    "    \n",
    "    word.start_time.seconds += secs\n",
    "    word.end_time.seconds += secs\n",
    "\n",
    "    word.start_time.nanos = (word.start_time.nanos + nanos ) % 10**9\n",
    "    word.end_time.nanos = (word.end_time.nanos + nanos ) % 10**9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#### START ####\n",
    "\n",
    "a = input('hardcode input (default)? or manual(m)? ')\n",
    "\n",
    "if(a == 'm'):\n",
    "    a = input('enter Google storage location e.g: gs://ringr_audio/venv/RawAudio/case1.wav : ')\n",
    "    storage_uri = a\n",
    "    a = input('enter local file location e.g: RawAudio/case1.wav : ')\n",
    "    sr, case2 = wavfile.read(a)\n",
    "    \n",
    "else:\n",
    "    # ONLY CHANGE THESE VARLAIBELS\n",
    "    storage_uri = 'gs://ringr_audio/venv/RawAudio/case1.wav'                                 # google cloud API\n",
    "    #storage_uri = 'gs://ringr_audio/venv/wetransfer-9e/case1.wav' \n",
    "    sr, case2 = wavfile.read('RawAudio/case1.wav')                                           # local read\n",
    "\n",
    "\n",
    "# no user inputon this    \n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"JSON/My First Project-1534988de9b5.json\"   # JSON API KEY\n",
    "# do transcription    \n",
    "transcript = sample_long_running_recognize(storage_uri)\n",
    "\n",
    "# translate API transcript to one easier to use\n",
    "gtranscript = Transcript()\n",
    "gtranscript.setupGoogle(transcript)\n",
    "gtranscript.initAudio(case2, sr)\n",
    "\n",
    "\n",
    "# audio widget\n",
    "sound(case2, sr, 'old sound')\n",
    "\n",
    "# GUI TESTING\n",
    "print('Launching UI')\n",
    "qApp = QtWidgets.QApplication(sys.argv)\n",
    "\n",
    "# aw = ApplicationWindow, currently using TranscriptEditor from qtdesign\n",
    "aw = Ui_TranscriptEditor()\n",
    "aw.setupUi(aw)\n",
    "aw.setupUiManual()\n",
    "\n",
    "spec = stft(input_sound=case2, dft_size=256, hop_size=64, zero_pad=256, window=signal.hann(256))\n",
    "t,f = FormatAxis(spec, sr, len(case2)/sr)\n",
    "\n",
    "tarray = [gtranscript, gtranscript]\n",
    "aw.launchInit(spec, t, f, tarray, 2)\n",
    "\n",
    "aw.show()\n",
    "sys.exit(qApp.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
